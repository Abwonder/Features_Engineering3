{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2cf538",
   "metadata": {},
   "source": [
    "## Feature Advance 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346e5f4",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "__answer__\n",
    "\n",
    "__Min-Max scaling__ is a data normalization technique used in data preprocessing that scales the features of a dataset to a fixed range, typically between 0 and 1. This is done by subtracting the minimum value of the feature and then dividing the result by the difference between the maximum and minimum values of the feature.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_feature = (feature - min_feature) / (max_feature - min_feature)\n",
    "\n",
    "where:\n",
    "\n",
    "feature: the original value of the feature\n",
    "\n",
    "min_feature: the minimum value of the feature\n",
    "\n",
    "max_feature: the maximum value of the feature\n",
    "\n",
    "scaled_feature: the normalized value of the feature\n",
    "\n",
    "__Usefulness__\n",
    "\n",
    "Min-Max scaling is useful for features that have different ranges and units of measurement, as it allows them to be compared on the same scale. It also helps to avoid issues with algorithms that assume that the data is centered around zero or that have different sensitivity to the scale of the features.\n",
    "\n",
    "\n",
    "__Example of real-world application__\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset with a feature \"age\" ranging from 20 to 60 and a feature \"income\" ranging from 30,000 to 90,000. We want to apply Min-Max scaling to these features.\n",
    "\n",
    "The minimum value of \"age\" is 20 and the maximum value is 60. The minimum value of \"income\" is 30,000 and the maximum value is 90,000.\n",
    "\n",
    "To scale the \"age\" feature, we subtract the minimum value of 20 from each value and then divide by the difference between the maximum value of 60 and the minimum value of 20, which is 40. So, for example, if someone is 30 years old, the scaled value of their age would be (30 - 20) / 40 = 0.25.\n",
    "\n",
    "To scale the \"income\" feature, we subtract the minimum value of 30,000 from each value and then divide by the difference between the maximum value of 90,000 and the minimum value of 30,000, which is 60,000. So, for example, if someone's income is 60,000, the scaled value of their income would be (60,000 - 30,000) / 60,000 = 0.5.\n",
    "\n",
    "After applying Min-Max scaling, both features will have values between 0 and 1, which allows them to be compared on the same scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0314c28",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "__Answer__\n",
    "\n",
    "__The Unit Vector technique__ is a data normalization technique used in feature scaling that scales the magnitude of each feature vector to 1. It is also known as \"Normalization\" or \"L2 normalization\".\n",
    "\n",
    "The Unit Vector technique is applied by dividing each feature vector by its Euclidean length, which is the square root of the sum of the squares of its individual values. This ensures that each feature vector has a length of 1, while preserving the direction of the original feature vector.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "scaled_feature_vector = feature_vector / ||feature_vector||\n",
    "\n",
    "where:\n",
    "\n",
    "feature_vector: the original vector of the feature\n",
    "\n",
    "||feature_vector||: the Euclidean length of the feature vector\n",
    "\n",
    "scaled_feature_vector: the normalized vector of the feature\n",
    "\n",
    "__The Difference between Min-Max scaling and Unit Vector scaling__\n",
    "\n",
    "The main difference between Min-Max scaling and Unit Vector scaling is that Min-Max scaling scales the feature values to a fixed range, while Unit Vector scaling scales the feature vectors to a fixed magnitude.\n",
    "\n",
    "\n",
    "__Ellustration Assignment__\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose we have a dataset with two features: \"age\" and \"income\". Each data point is represented as a vector in a 2-dimensional space, where the x-axis represents \"age\" and the y-axis represents \"income\". We want to apply Unit Vector scaling to these features.\n",
    "\n",
    "For example, suppose we have a data point with \"age\" = 40 and \"income\" = 60,000. The corresponding feature vector would be (40, 60,000).\n",
    "\n",
    "The Euclidean length of the feature vector is calculated as follows:\n",
    "\n",
    "||feature_vector|| = sqrt(40^2 + 60,000^2) = 60,000.05\n",
    "\n",
    "To scale the feature vector using Unit Vector scaling, we divide each component of the vector by its Euclidean length:\n",
    "\n",
    "scaled_feature_vector = (40 / 60,000.05, 60,000 / 60,000.05) = (0.00066667, 0.99999996)\n",
    "\n",
    "After applying Unit Vector scaling, the feature vector now has a length of 1, while preserving its direction. This makes it easier to compare the magnitudes of the feature vectors across different data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bdb1e",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "__Answer__ \n",
    "\n",
    "__PCA (Principle Component Analysis)__ is a technique used in machine learning and data analysis for dimensionality reduction. It is a mathematical method that transforms a large set of correlated variables into a smaller set of uncorrelated variables called principal components, while still retaining the most important information in the original dataset.\n",
    "\n",
    "The main goal of PCA is to reduce the dimensionality of the dataset, which can help improve the efficiency of algorithms, reduce the risk of overfitting, and simplify the data visualization process.\n",
    "\n",
    "PCA works by finding the directions of maximum variance in a dataset and projecting the data onto these directions. The first principal component captures the direction with the highest variance, while the second principal component captures the direction with the second-highest variance, and so on. The number of principal components retained is determined by a threshold or by analyzing the explained variance ratio.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset of 1000 data points with 5 features: age, income, education, work experience, and location. We want to reduce the dimensionality of this dataset using PCA.\n",
    "\n",
    "1. First, we standardize the dataset by subtracting the mean of each feature and dividing by its standard deviation. This step is important to ensure that each feature has equal importance in the PCA analysis.\n",
    "\n",
    "2. Next, we calculate the covariance matrix of the standardized dataset. The covariance matrix represents the degree of correlation between each pair of features.\n",
    "\n",
    "3. Then, we calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, while the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. We select the top k principal components based on their corresponding eigenvalues or the explained variance ratio. For example, if we want to retain 2 principal components, we select the first 2 eigenvectors with the highest eigenvalues.\n",
    "\n",
    "5. Finally, we project the original dataset onto the selected principal components. This creates a new dataset with fewer features, where each data point is represented by its corresponding principal component scores.\n",
    "\n",
    "__Finally__\n",
    "\n",
    "The resulting dataset can be used for further analysis, such as clustering or classification, or for visualization purposes. By reducing the number of features, PCA can help simplify the dataset and improve the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bfb55",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "\n",
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "__Answer__\n",
    "\n",
    "__PCA and feature extraction__ are related concepts in machine learning and data analysis. Feature extraction is the process of selecting or transforming features from the original dataset to create a new set of features that are more informative or easier to work with. PCA can be used as a feature extraction technique to transform a large set of correlated features into a smaller set of uncorrelated features while still preserving most of the information in the original dataset.\n",
    "\n",
    "In PCA, the principal components are the new features created by linear combinations of the original features. Each principal component represents a new feature that captures the most important information or variation in the original dataset. These new features can be used as inputs to machine learning algorithms or for data visualization purposes.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of 1000 images, each with 100x100 pixels. Each pixel represents a feature, resulting in a dataset with 10,000 features per image. This high-dimensional dataset can be difficult to work with and may lead to overfitting or slow performance of machine learning algorithms.\n",
    "\n",
    "To reduce the dimensionality of the dataset, we can use PCA as a feature extraction technique. First, we flatten each image into a 1-dimensional vector of 10,000 features. Then, we standardize the dataset by subtracting the mean of each feature and dividing by its standard deviation.\n",
    "\n",
    "Next, we apply PCA to the standardized dataset. We select the top k principal components based on their corresponding eigenvalues or the explained variance ratio. For example, if we want to retain 50 principal components, we select the first 50 eigenvectors with the highest eigenvalues.\n",
    "\n",
    "Finally, we project each image onto the selected principal components to obtain a new set of features for each image. Each image is now represented by a vector of 50 features, which capture the most important information or variation in the original image.\n",
    "\n",
    "These new features can be used as inputs to machine learning algorithms for tasks such as image classification or clustering. By reducing the dimensionality of the dataset and extracting informative features, PCA can help improve the performance and efficiency of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33e854",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "__Answer__\n",
    "\n",
    "To use Min-Max scaling to preprocess the data for a recommendation system for a food delivery service, we would first need to identify the features that need to be scaled. In this case, the features that require scaling are likely to be price, rating, and delivery time.\n",
    "\n",
    "Min-Max scaling is a common normalization technique used to rescale numeric data in a way that all values fall within a specified range, typically between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original value \n",
    "\n",
    "X_min is the minimum value in the dataset\n",
    "\n",
    "X_max is the maximum value in the dataset.\n",
    "\n",
    "__Application__\n",
    "\n",
    "To apply Min-Max scaling to the food delivery dataset, we would first calculate the minimum and maximum values for each feature. For example, we might find that the minimum price is 5 and the maximum price is 50, the minimum rating is 1 star and the maximum rating is 5 stars, and the minimum delivery time is 10 minutes and the maximum delivery time is 60 minutes.\n",
    "\n",
    "Then, we would use the Min-Max scaling formula to rescale each value for each feature. For example, if we have a food item with a price of $10, a rating of 4 stars, and a delivery time of 30 minutes, the scaled values would be:\n",
    "\n",
    "\n",
    "Scaled price = (10 - 5) / (50 - 5) = 0.15\n",
    "\n",
    "Scaled rating = (4 - 1) / (5 - 1) = 0.75\n",
    "\n",
    "Scaled delivery time = (30 - 10) / (60 - 10) = 0.43\n",
    "\n",
    "After applying Min-Max scaling to all the features in the dataset, we would have a normalized dataset where all values fall within the range of 0 to 1. This can help improve the performance of machine learning algorithms that use the dataset as input, as it ensures that all features have equal importance and are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c82ef5",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "__Answer__\n",
    "\n",
    "PCA can be a useful technique to reduce the dimensionality of a dataset with many features, such as the financial data and market trends used in a stock price prediction model. Here is a general approach to using PCA for this purpose:\n",
    "\n",
    "1. __Standardize the data:__ Before applying PCA, we should standardize the data to ensure that each feature has the same scale and mean. This can be done by subtracting the mean of each feature and dividing by its standard deviation.\n",
    "\n",
    "2. __Compute the covariance matrix:__ The covariance matrix measures how each pair of features varies together. It is a square matrix with dimensions equal to the number of features in the dataset. We can compute the covariance matrix using the standardized data.\n",
    "\n",
    "3. __Compute the eigenvectors and eigenvalues:__ The eigenvectors and eigenvalues of the covariance matrix represent the direction and magnitude of the principal components of the data. We can compute these using a matrix decomposition technique, such as Singular Value Decomposition (SVD) or Eigendecomposition.\n",
    "\n",
    "4. __Select the number of principal components:__ We can choose the number of principal components to retain based on the explained variance ratio or the cumulative sum of the eigenvalues. For example, we might choose to retain the top k principal components that explain 90% of the variance in the data.\n",
    "\n",
    "5. __Transform the data:__ Finally, we can transform the original data into a lower-dimensional space by multiplying it with the selected eigenvectors. The resulting transformed data has fewer dimensions, but it still captures most of the information or variation in the original data.\n",
    "\n",
    "In the case of a stock price prediction model, we might apply PCA to the financial data and market trends features to reduce their dimensionality. This would result in a smaller set of features that capture the most important information or trends in the data, and could be used as input to a machine learning model. By reducing the dimensionality of the data, we can reduce the risk of overfitting and improve the performance of the model. However, it is important to note that PCA is not always the best approach, and it is important to evaluate the trade-offs between dimensionality reduction and information loss in the specific context of the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9746a",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "__Answer__\n",
    "\n",
    "check the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e86b7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.21052631578947367, 0.47368421052631576, 0.7368421052631579, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# first method\n",
    "## dataset to apply min-max scaling on\n",
    "data = [1, 5, 10, 15, 20]\n",
    "data_scaled = []\n",
    "max_num = max(data)\n",
    "min_num = min(data)\n",
    "\n",
    "for num in data:\n",
    "    scale = (num - min_num)/(max_num - min_num)\n",
    "    data_scaled.append(scale)\n",
    "#     print(num)\n",
    "print(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc6c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "796b72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a785436e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### I am not sure why this is not working put this the right way to do it\n",
    "min_max.fit_transform([[1, 5, 10, 15, 20]])\n",
    "\n",
    "# Note: the first example worked, method I mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d538a",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "__Answer__\n",
    "\n",
    "I would retain the number of compenent that explain atleast 80% of the total variance of the dataset. I will follow the following steps:\n",
    "\n",
    "1. Perform a PCA \n",
    "2. plot the explained variance ratio for each component (scree plot elbow)\n",
    "3. choose the minimum number of components that capture at least 95% of the total variance.\n",
    "\n",
    "I choose 3 component from the random data I used below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edd8e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# simulate data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "height = np.random.normal(170, 10, n_samples)\n",
    "weight = np.random.normal(70, 15, n_samples)\n",
    "age = np.random.normal(40, 10, n_samples)\n",
    "gender = np.random.binomial(1, 0.5, n_samples)\n",
    "blood_pressure = np.random.normal(120, 10, n_samples)\n",
    "\n",
    "# combine data into a matrix\n",
    "X = np.column_stack((height, weight, age, gender, blood_pressure))\n",
    "\n",
    "# create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# standardize the data\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# create a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model\n",
    "pca.fit(X_std)\n",
    "\n",
    "# compute the explained variance ratio for each component\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "# plot the explained variance ratio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(explained_var)+1), explained_var, 'o-')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.title(\"Screeplot\", loc=\"left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "105de683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.04040409, -0.94713976,  0.57791341],\n",
       "       [ 0.69333369, -1.39401286, -0.55367771],\n",
       "       [-0.40272265,  0.66553009, -0.5643062 ],\n",
       "       ...,\n",
       "       [-0.20059263,  0.41229868,  1.28277885],\n",
       "       [-0.30182067,  0.29657088, -1.38097591],\n",
       "       [-0.91024224,  1.14136595,  0.24955197]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the number of components chose base ont the scree plot\n",
    "n_components = 3\n",
    "\n",
    "# transform the data\n",
    "X_pca = pca.transform(X_std)\n",
    "X_pca[:, :n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9e287",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
